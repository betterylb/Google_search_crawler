Google Search Crawler
======================

This is a simple google search results crawler. Before use this tool, please read these tips below.

Requirements
----------------------
1. Python

    python should be installed in your computer. here is the official website: http://www.python.org
2. BeautifulSoup

    A html parser to extract search results from Google. BeautifulSoup(version 4) is better.

    For more information about BeautifuleSoup, please visit: http://www.crummy.com/software/BeautifulSoup/

How to Use?
----------------------
1.修改Configure.py中的代理配置选项，配置好相应的代理ip和端口


2. 导入库类文件
    from gsearch import ThreadCrawl

3. 创建ThreadCrawl对象，创建对象时需要传入3个参数
    expect_num：需要爬取的网页数
    keyword：需要爬取的关键词
    store：本地存储的盘符
    例子：
        假如想要爬取50个包含关键词Facebook的网页，爬取的网页存储到本地的D磁盘，需创建对象如下：
        crawler = ThreadCrawl(50, "Morgan Freeman", "D")

4. 运行所创建对象的函数
    crawler.start()

5. 输出结果
    程序运行完毕后会在D磁盘生成D:\google_search_result\Morgan Freeman的文件夹
    文件夹内含有数个子文件夹，子文件夹名字如下：web_1,web_2,web_3,....。
    每个子文件夹含有一个html文档（所爬取的网页），html文档以所爬取网页的
    网址命名，例如：www.nbcnews.com